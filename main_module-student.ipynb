{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h6SINw7KfE3"
   },
   "source": [
    "# Week 6: Dimensionality Reduction - PCA and UMAP\n",
    "This week, we will study **dimensionality reduction**, and applications of **principal component analysis (PCA)** and **Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkSEUe9kIgDc"
   },
   "source": [
    "## Learning objectives\n",
    "In this module, you will learn:\n",
    "1. Sketch the direction of the first principal component for some given data in a 2D space\n",
    "2. Explain why it is crucial to standardize the data before applying PCA\n",
    "3. Apply PCA in Python using **`scikit-learn`**\n",
    "4. Understand when to use PCA vs. UMAP for dimensionality reduction\n",
    "5. Apply UMAP and compare its results with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIwU0nDNIhs3"
   },
   "source": [
    "## Breast Cancer Detection through Image Recognition\n",
    "\n",
    "Suppose we are analyzing a data set of breast cancer histopathology images. We have provided two images below. The left image contains benign breast cancer issues, whereas the right image contains malignant breast cancer tissues. **Our goal is to determine whether an image represents benign or malignant breast cancer issues.** How would you perform this prediction?\n",
    "\n",
    "<img src=\"download.jpeg\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "\n",
    "You might notice that benign and breast cancer tissues have distinct characteristics, such as different ductal structures, nuclear morphologies, cell morphologies, and so on. **We need a way to determine which of these characteristics is most important in classifying a sample as malignant (a tumor) or benign.**\n",
    "\n",
    "In a computer, images are represented as a 2D grid of pixels. Pixels have values that encode a specific colour using systems like RGB. Likewise, a patch in a breast cancer image, say of a tumour, will have similar pixel values, indicating that these pixels represent the same structure.\n",
    "\n",
    "We often consider each pixel of an image as a **feature** (or a **variable**, which you might be more familiar with). The value of each feature is the color of the pixel converted to a number.\n",
    "\n",
    "Although each image has many pixels, **only some** pixels are important for determining whether the breast cancer issue is benign or malignant. We want to figure out and isolate the important pixels.\n",
    "\n",
    "With image analysis tasks, we would typically use machine learning architectures like [convolutional neural networks (CNN)](https://www.ibm.com/think/topics/convolutional-neural-networks). However, this isn't the only way to solve a problem; oftentimes, limited resources or computing power necessitate alternative analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to Image Analysis and Dimensionality Reduction\n",
    "\n",
    "In a current clinical setting, pathologists and radiologists manually record measurements of breast cancer images, like the size, radius, density of the image, and so on. Key medical decisions are made with this data! For this exercise, we are going to look at a dataset containing measurements that doctors took from breast cancer images rather than looking at the images themselves--these measurements are the **features** in our dataset. In a future module, when we go over computer vision pipelines, we will investigate vision-based techniques.\n",
    "\n",
    "**Our ultimate goal is to preserve all the relevant features that are required for classification and remove all the rest**, ultimately reducing the dimensions of the data. The process of reducing the number of features of a dataset, while preserving the most important information in the dataset, is called **dimensionality reduction**.\n",
    "\n",
    "To explore our problem, we will use the [Diagnostic Wisconsin Breast Cancer Database](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). This dataset is readily callable from `sklearn.datasets` by calling `load_breast_cancer()`, and is a popular dataset used by aspiring machine learning scientists to test their models and learn. This dataset contains features extracted from images of breast cancer biopsies. Analyzing actual breast cancer histopathology images would require a lot of computing power since these images are **extremely** high-dimensional (i.e., have a lot of pixels), so this will work in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYSQs8sJxBdg"
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGERppE6Q67Y"
   },
   "outputs": [],
   "source": [
    "!pip install -q umap-learn\n",
    "\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd # For data manipulation and analysis\n",
    "\n",
    "import matplotlib.pyplot as plt   #  For plotting\n",
    "import matplotlib as mpl    # For configuring the plotting\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "# sklearn for machine learning\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# UMAP for non-linear dimensionality reduction\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8grzaNhxBdg"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "print(data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCh8n46qxBdh"
   },
   "source": [
    "---\n",
    "**Q*1. How many features does the dataset have? Give 4 examples of the features.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K2zpfXRxBdh"
   },
   "source": [
    "Even though 30 features is much less than the number of pixels in an image, it's still difficult to visualize 30 dimensions. Instead, we can use dimensionality reduction techniques like PCA and UMAP to bring the data down into two dimensions to visualize on a plot, which we will explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9GJqHklIpk7"
   },
   "source": [
    "## Part 1: Introduction to Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nBDcat0G2lX"
   },
   "source": [
    "Principal Component Analysis (PCA) aims to preserve the features that \"explain the most variance\". We would therefore expect the principal components that the data is reduced to, to correspond to the largest \"spread\" or variance in data points. Variance is a measurement of how data points differ from the mean.\n",
    "\n",
    "**A Toy Example**\n",
    "\n",
    "Before we attempt PCA on the `sklearn` breast cancer dataset, let's use a toy example to gain some intuition about PCA. The code below generates a dataset with two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSCMASlXItZM"
   },
   "outputs": [],
   "source": [
    "# Let's generate a toy 2D dataset with a strong linear relationship\n",
    "np.random.seed(42)\n",
    "X_new = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T\n",
    "X_new = X_new - X_new.mean(axis=0)  # center data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was generated by adding noise to a 45˚ line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsX_37GAUdKT"
   },
   "source": [
    "The code below plots the noisy data points that we generated from the 45˚ line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6ProzdQUKyi"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Generated Toy Data')\n",
    "plt.axis('equal')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2Vsh5u0CaTJ"
   },
   "source": [
    "Next, let's explore the process of reducing our 2D data to 1D. Imagine selecting a line and projecting all data points onto it. This involves visualizing the movement of each data point to the line and measuring the projection error, defined as the distance each point travels to reach the line. Ideally, we aim to select a line that explains the most variance, thereby minimizing the distance of each point from the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function that projects the data onto a line with a slope of your choice and an intercept of 0. It also returns the average distance of the points from the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-bN4fAtCu8f"
   },
   "outputs": [],
   "source": [
    "def plot_projection(m, title=\"Projection onto a Line\"):\n",
    "    # Calculate projection points on the line for each X_new point (i.e., the distance each point must travel)\n",
    "    X_line = np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 100)\n",
    "    Y_line = m * X_line\n",
    "    X_proj = (m * (X_new[:, 1]) + X_new[:, 0]) / (m**2 + 1)\n",
    "    Y_proj = (m**2 * X_new[:, 1] + m * X_new[:, 0]) / (m**2 + 1)\n",
    "\n",
    "    error = np.sqrt((X_new[:, 0] - X_proj)**2 + (X_new[:, 1] - Y_proj)**2)\n",
    "    projected_point_spread = np.sqrt((X_proj - X_proj.mean())**2 + (Y_proj - Y_proj.mean())**2)\n",
    "\n",
    "    # Plot data (same as above!)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5, label='Original Data')\n",
    "    # Plot the line (the line we defined with slope and intercept)\n",
    "    plt.plot(X_line, Y_line, color='red', label='Projection Line')\n",
    "    # Plot projections (our distances!)\n",
    "    for i in range(len(X_new)):\n",
    "        plt.plot([X_new[i, 0], X_proj[i]], [X_new[i, 1], Y_proj[i]], 'k--')\n",
    "    plt.scatter(X_proj, Y_proj, color='green', label='Projected Points', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    print(f\"Average distance: {np.mean(error)}\")\n",
    "    print(f\"Projected point spread: {np.mean(projected_point_spread)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different slopes. First, try a slope of 0, being the horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(0, \"Projection onto a horizontal line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a slightly larger slope, like 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(0.2, \"Projection onto a line with a slope of 0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJCIgXqZFpxE"
   },
   "source": [
    "---\n",
    "**Q*2: Suppose our goal is to retain the biggest spread or variance in the projected data points along the line. Which of the two lines above is better, and why?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q*3: Now, suppose our goal is to minimize the average distance of the projected points to their original points. Which of the two lines above is better, and why?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q*4: Now consider both goals. What is the best line to project the data onto? Use the provided function to help you answer this question.**\n",
    "> Hint: Consider the question \"How did we generate our data?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(1, \"Projection onto a line with a slope of 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q*5. Still considering both goals, what is likely the worst line to project the data onto? Use the provided function to help you answer this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(-1, \"Projection onto a line with a slope of -1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsz-ypXuI8y3"
   },
   "source": [
    "After exploring how projections onto different lines can vary in their effectiveness at capturing data variance, we're now ready to introduce **Principal Component Analysis (PCA)**.\n",
    "\n",
    "Essentially, PCA is a systematic method to identify the **most informative projection directions**—those that maximize variance and, hence, capture the data's inherent structure. Unlike our earlier manual explorations with lines, PCA automates this search and quantifies the importance of each direction.\n",
    "\n",
    "In this section, we apply PCA to our toy data to visually understand how PCA identifies principal components. These components are the best lines we can project our data onto if we want to preserve as much information as possible.\n",
    "\n",
    "The principal components show us not just any lines but the ones along which the variance of our data is maximized. This is basically a way to automatically find the optimal projection line, with a data-driven approach.\n",
    "\n",
    "Let's take a look at how PCA works on our toy data. We'll plot the original data points and overlay the principal components as vectors. These vectors represent the directions of maximum variance, with their length indicative of the variance magnitude explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05Bxc3hPVcmd"
   },
   "outputs": [],
   "source": [
    "# StandardScaler is used to standardize features by removing the means and adjust to the variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use StandardScaler to scale the features\n",
    "# We will explain why scaling data is necessary in PCA later\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_new)\n",
    "\n",
    "# Apply PCA to toy data\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d.fit(X_scaled)\n",
    "\n",
    "# Plot the original toy data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5)\n",
    "\n",
    "# Add the principal components to the plot as direction and magnitude arrows\n",
    "for length, vector in zip(pca_2d.explained_variance_, pca_2d.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.quiver(pca_2d.mean_[0], pca_2d.mean_[1], v[0], v[1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Principal Components on Toy Data')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOMFG74YIIcg"
   },
   "source": [
    "### Scaling in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwRSWjK5IJ8S"
   },
   "source": [
    "In our earlier implementation, before applying PCA, we scaled the data. Now, let's explain why.\n",
    "\n",
    "Scaling is a critical preprocessing step for PCA, especially when your features have different units and scales. This is because PCA looks for directions of maximum variance to identify principal components. Variance is heavily influenced by the scale of the features. If one feature has a much larger scale than others, PCA might misleadingly consider that feature bing more important, not because of any intrinsic property of that feature, but simply due to its larger numeric range.\n",
    "\n",
    "Let's look at a simple example. Given a dataset with two features:\n",
    "-  Feature 1: ranges from 0 to 1\n",
    "-  Feature 2: ranges from 0 to 100\n",
    "    \n",
    "Without scaling, PCA can overemphasize the variance along the direction of larger numerical ranges, which is Feature 2. If plotting the PCA, the principal component would lie almost entirely along the axis of Feature 2.\n",
    "Consequently, the variance captured by PCA is overwhelmingly influenced by Feature 2 and overshadowing any contribution from Feature 1. This leads to biases towards Feature 2.\n",
    "Feature 1 may be equally important; however, PCA ignores Feature 1 because its range is small and its influence on variance is small before scaling.\n",
    "\n",
    "With scaling, PCA gives a more balanced and accurate representation of the data's intrinsic structure, highlighting the true directions that maximize variance. Scaling before applying PCA ensures that all features contribute equally to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJavaGQeRSH3"
   },
   "source": [
    "### Applying PCA to the Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jiaq_TlxBdj"
   },
   "source": [
    "Now let's go back to the breast cancer dataset that was introduced at the beginning.\n",
    "\n",
    "Let's prepare the data for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuNUMFCcxBdj"
   },
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt6zKDakxBdj"
   },
   "outputs": [],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7WEXRFLxBdj"
   },
   "source": [
    "Starting with standardizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRpPzplJRPGA"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7AH0mAFd1Uv"
   },
   "outputs": [],
   "source": [
    "# We're setting the number of components to 2 to get a 2D visualization by changing value of n_components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR0UsmBuSo2p"
   },
   "outputs": [],
   "source": [
    "# Look at the PCA Results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='plasma', edgecolor='k', s=40)\n",
    "plt.title('2D PCA of Breast Cancer Dataset')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='indigo', label='Malignant', markersize=8),\n",
    "                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', label='Benign', markersize=8)],\n",
    "          title='Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hO7fAbpRguZS"
   },
   "source": [
    "As we coloured the samples by target ID, we can see that the data appears separable based on the target label. We could draw a line on this graph to separate cancerous and non-cancerous samples (our malignant vs. benign labelled histopathologies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JITdf83ueYop"
   },
   "source": [
    "Let's see how much variance the first two PCA components explain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jO9DipP2e2LQ"
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "print(f\"Total Variance Explained: {sum(explained_variance):.2f} or {sum(explained_variance)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8lZc5UPSymW"
   },
   "source": [
    "### Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiOr6T68Jnyt"
   },
   "source": [
    "*How do we ensure our dimensionality reduction keeps the most important information?*\n",
    "\n",
    "There are many techniques for dimensionality reduction that aim to preserve the most important information. In the case of PCA, we will be learning how to preserve the features that \"explain the most variance\". This means we will be summarizing the data using the features that explain **the most differences** in our dataset, which ideally represent the labels of the classification task.\n",
    "\n",
    "Usually, in dimensionality reduction, when we transform higher-dimensional data (data with a lot of features) into a lower-dimensional space (the summary), each dimension in the lower-dimensional space represents a combination of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSrUn5eDfeTv"
   },
   "source": [
    "Let's take a look at what composition of features these components have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTpwtZlYSujx"
   },
   "outputs": [],
   "source": [
    "# Look at the composition of the first two principal components.\n",
    "pca_components = pd.DataFrame(pca.components_, columns=feature_names)\n",
    "print(pca_components.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6N-rJugYUBaY"
   },
   "outputs": [],
   "source": [
    "# Visualize the composition of the first two components\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i in range(2):\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    pca_components.loc[i].plot(kind='bar')\n",
    "    plt.title(f'Principal Component {i+1}')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQD9ux_jUQ3f"
   },
   "source": [
    "The PCA component weights reveal the relative importance of each feature in the dataset's variance. For instance, if texture has a high weight in the first principal component, it suggests that texture variation significantly contributes to distinguishing between different observations in the dataset. This might reflect the biological reality where texture variations in histopathology images are crucial for distinguishing between benign and malignant breast cancer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Uniform Manifold Approximation and Projection (UMAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to UMAP\n",
    "\n",
    "While PCA is a powerful linear dimensionality reduction technique, it has limitations when dealing with data that has complex, non-linear relationships. UMAP (Uniform Manifold Approximation and Projection) is a more recent dimensionality reduction technique that can capture both local and global structure in your data. UMAP is a stochastic algorithm, so results can vary between runs (though this can be controlled with the `random_state` parameter).\n",
    "\n",
    "The key differences between PCA and UMAP are:\n",
    "\n",
    "| Aspect | PCA | UMAP |\n",
    "|--------|-----|------|\n",
    "| Type | Linear | Non-linear |\n",
    "| Preserves | Global structure (variance) | Local and global structure |\n",
    "| Computation | Fast | Moderate (faster than t-SNE) |\n",
    "| Interpretability | Components have clear meaning | Dimensions lack direct interpretation |\n",
    "| Scalability | Scales well | Scales better than other non-linear methods |\n",
    "| Use case | Feature extraction, noise reduction | Visualization, clustering |\n",
    "\n",
    "Let's apply UMAP to our breast cancer dataset and compare it with our earlier PCA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP to the scaled data\n",
    "reducer = umap.UMAP(random_state=42, n_components=2, n_jobs=1)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the UMAP results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='plasma', edgecolor='k', s=40)\n",
    "plt.title('2D UMAP of Breast Cancer Dataset')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='indigo', label='Malignant', markersize=8),\n",
    "                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', label='Benign', markersize=8)],\n",
    "          title='Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that UMAP has created a clearer separation between the classes compared to PCA. This is because UMAP is better at preserving both local and global structure in the data, making it particularly effective for visualization tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning UMAP Parameters\n",
    "\n",
    "UMAP offers several important parameters that influence the resulting dimensionality reduction:\n",
    "\n",
    "1. **`n_neighbors`**: Controls the balance between local and global structure. Higher values (e.g., 30-50) result in more focus on global structure, while lower values (e.g., 5-15) emphasize local structure.\n",
    "\n",
    "2. **`min_dist`**: Controls how tightly points are packed together. Lower values (such as 0.01) create tighter clusters, while higher values (such as 0.5) create more dispersed representations.\n",
    "\n",
    "Let's see how these parameters affect dimensionality reduction using different parameter combinations in the code cell below.\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different parameter combinations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Different n_neighbors values\n",
    "# TODO: Try out different values for n_neighbors and cpmpare the resulting graphs\n",
    "for i, n in enumerate([..., ...]):\n",
    "    reducer = umap.UMAP(n_components=2, n_jobs=1,n_neighbors=n, min_dist=0.1, random_state=42)\n",
    "    result = reducer.fit_transform(X_scaled)\n",
    "    scatter = axes[0, i].scatter(result[:, 0], result[:, 1], c=y, cmap='plasma', s=20)\n",
    "    axes[0, i].set_title(f'n_neighbors = {n}')\n",
    "    axes[0, i].set_xlabel('UMAP Dimension 1')\n",
    "    axes[0, i].set_ylabel('UMAP Dimension 2')\n",
    "\n",
    "# Different min_dist values\n",
    "# TODO: try out different values for min_dist and compare the resulting graphs\n",
    "for i, d in enumerate([..., ...]):\n",
    "    reducer = umap.UMAP(n_components=2, n_jobs=1, n_neighbors=15, min_dist=d, random_state=42)\n",
    "    result = reducer.fit_transform(X_scaled)\n",
    "    scatter = axes[1, i].scatter(result[:, 0], result[:, 1], c=y, cmap='plasma', s=20)\n",
    "    axes[1, i].set_title(f'min_dist = {d}')\n",
    "    axes[1, i].set_xlabel('UMAP Dimension 1')\n",
    "    axes[1, i].set_ylabel('UMAP Dimension 2')\n",
    "\n",
    "# Add a single legend for all plots\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='indigo', label='Malignant', markersize=8),\n",
    "           plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', label='Benign', markersize=8)]\n",
    "fig.legend(handles=handles, title='Target', loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be exploring the effect of these parameters more during **Clustering II** next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing PCA and UMAP\n",
    "\n",
    "Now, let's directly compare PCA and UMAP on the breast cancer dataset to see how they differ in their ability to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and UMAP with default parameters\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize both projections side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PCA\n",
    "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='plasma', s=30)\n",
    "ax1.set_title('PCA projection')\n",
    "ax1.set_xlabel('Principal Component 1')\n",
    "ax1.set_ylabel('Principal Component 2')\n",
    "\n",
    "# UMAP\n",
    "scatter2 = ax2.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='plasma', s=30)\n",
    "ax2.set_title('UMAP projection')\n",
    "ax2.set_xlabel('UMAP Dimension 1')\n",
    "ax2.set_ylabel('UMAP Dimension 2')\n",
    "\n",
    "# Add a legend instead of colorbar\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='indigo', label='Malignant', markersize=8),\n",
    "           plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', label='Benign', markersize=8)]\n",
    "fig.legend(handles=handles, title='Target', loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use UMAP vs PCA\n",
    "\n",
    "**Use PCA when**:\n",
    "- You need interpretable components\n",
    "- You want to reduce dimensionality while preserving variance for downstream tasks like regression or classification\n",
    "- Computational efficiency is a priority\n",
    "- Your data has a primarily linear structure\n",
    "\n",
    "**Use UMAP when**:\n",
    "- You want to visualize high-dimensional data in 2D or 3D\n",
    "- Your data likely has non-linear relationships\n",
    "- You need to preserve both local and global structure\n",
    "- Cluster separation is important for your analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of UMAP\n",
    "\n",
    "While UMAP is powerful, it has limitations:\n",
    "- Unlike PCA, the dimensions in UMAP don't have a clear interpretation\n",
    "- UMAP is stochastic, so results can vary between runs (though this can be controlled with the `random_state` parameter)\n",
    "- The resulting representation can be sensitive to parameter choices\n",
    "- UMAP prioritizes preserving topology over preserving distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB5q9YeXOFfe"
   },
   "source": [
    "## **Graded Exercise: (5 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlnTYp6RL9VD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**GQ*1. (1 mark) Diabetes Data PCA and UMAP**\n",
    "\n",
    "**In the cell block below, we have loaded in the Diabetes Dataset and printed out the description. Apply both PCA and UMAP to this dataset and plot the first two components/dimensions. Color the points by y.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQeC3xLWMAAR"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_data = load_diabetes()\n",
    "X_diabetes = diabetes_data.data\n",
    "y_diabetes = diabetes_data.target\n",
    "diabetes_feature_names = diabetes_data.feature_names\n",
    "\n",
    "# Print description\n",
    "print(diabetes_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq5wI87gNDb_"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "# Standardize the data\n",
    "\n",
    "\n",
    "# Apply PCA\n",
    "\n",
    "\n",
    "# Apply UMAP\n",
    "\n",
    "\n",
    "# Create side-by-side plot with space for colorbar\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "gs = plt.GridSpec(2, 2, height_ratios=[6, 1])\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "cbar_ax = plt.subplot(gs[1, :])\n",
    "\n",
    "# PCA plot\n",
    "\n",
    "\n",
    "# UMAP plot\n",
    "\n",
    "\n",
    "# Add horizontal colorbar\n",
    "cbar = plt.colorbar(scatter1, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('Diabetes Progression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1adAE3_NFV8"
   },
   "source": [
    "**GQ*2. (2 marks) Compare the PCA and UMAP results for the diabetes dataset. Do they show similar patterns? Which technique seems to better represent the data structure? Justify your answer.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xAj9Fu_NQ_o"
   },
   "source": [
    "**GQ*3. (2 marks) Looking at the PCA components and results from the diabetes dataset, explain whether PCA and UMAP seem appropriate for this dataset. What might be the limitations of using these techniques for this data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the composition of the first two principal components.\n",
    "pca_components = pd.DataFrame(pca.components_, columns=diabetes_feature_names)\n",
    "print(pca_components.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the composition of the first two components\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i in range(2):\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    pca_components.loc[i].plot(kind='bar')\n",
    "    plt.title(f'Principal Component {i+1}')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "print(f\"Total Variance Explained: {sum(explained_variance):.2f} or {sum(explained_variance)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0fouzrTiEsN"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this module, we explored two powerful dimensionality reduction techniques: PCA & UMAP.\n",
    "\n",
    "**PCA** is a linear technique that finds directions of maximum variance in the data. It's computationally efficient and produces interpretable components, making it ideal for feature extraction and noise reduction. However, it may struggle with data that has complex, non-linear relationships.\n",
    "\n",
    "**UMAP** is a non-linear technique that preserves both local and global structure in the data. It often creates better visualizations with clearer cluster separation, but its dimensions lack the clear interpretation that PCA components have.\n",
    "\n",
    "Good practices for dimensionality reduction include:\n",
    "1. Always scale your data before applying PCA or UMAP\n",
    "2. For UMAP, experiment with parameters like `n_neighbors` and `min_dist`\n",
    "3. Choose between PCA and UMAP based on your specific needs:\n",
    "   - Use PCA when you need interpretable components or computationally efficient reduction\n",
    "   - Use UMAP when you need better visualization or have non-linear data structures\n",
    "\n",
    "The choice between PCA and UMAP should be guided by your specific use case, the nature of your data, and what aspects of the data you want to preserve."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
