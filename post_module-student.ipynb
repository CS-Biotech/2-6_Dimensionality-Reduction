{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Post-Module Exercises - Optimal Component Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook focuses on a critical aspect of Principal Component Analysis (PCA): determining the optimal number of components to use. We'll also explore data reconstruction with PCA and how the number of components affects reconstruction quality.\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "\n",
    "# Configure plotting\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and prepare the breast cancer dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selecting the Optimal Number of PCA Components\n",
    "\n",
    "In the main module, we briefly touched on how explained variance can help determine the number of components to keep. Here, we'll explore this in more depth using the **elbow method** and **variance thresholds**.\n",
    "\n",
    "### How to think about the Explained Variance Ratio\n",
    "A dataset in general has $x$ amount of variance, and each variable contributes to that variance. The explained variance ratio aims to figure out the percentage of the variance that a component contributes to.\n",
    "\n",
    "### What is Cumulative Explained Variance\n",
    "If we were to order components by the variance that they contribute to, this gives us the **Cumulative Explained Variance** up to that point. For the sake of visualization, you want to maximize the cumulative explained variance and minimize the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA without limiting the number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance for each component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center', \n",
    "        label='Individual explained variance')\n",
    "plt.step(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), where='mid', \n",
    "        label='Cumulative explained variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Threshold')\n",
    "plt.axhline(y=0.9, color='g', linestyle='-', label='90% Threshold')\n",
    "plt.axhline(y=0.8, color='y', linestyle='-', label='80% Threshold')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Elbow Method for PCA\n",
    "\n",
    "The elbow method involves looking for the \"elbow\" point in the scree plot (the plot of explained variance), where the rate of decrease in explained variance slows down significantly. This point represents a good trade-off between dimensionality reduction and information preservation.\n",
    "\n",
    "A good way of trying to find the elbow is to find a datapoint where the preceding slope is steep and the slope after is far less steep. This corresponds to finding the point where the rate of decrease in explained variance slows down significantly, which is similar to finding the second derivative of the explained variance ratio.\n",
    "\n",
    "<img src=\"DataClustering_ElbowCriterion.JPG\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "\n",
    "In the plot above, we see that at the 4th component, we have the largest difference in adjacent slopes, explaining the change in the cumulative explained variance ratio, so we pick 4 to be the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method using second derivative\n",
    "def find_n_components_elbow(explained_variance_ratio):\n",
    "    # Calculate first differences (approximation of first derivative)\n",
    "    diffs = np.diff(explained_variance_ratio)\n",
    "    \n",
    "    # Calculate second differences (approximation of second derivative)\n",
    "    second_diffs = np.diff(diffs)\n",
    "    \n",
    "    # Find the first large spike in the second derivative\n",
    "    # (adding 2 because of the two different operations and one because of 0-indexing)\n",
    "    return np.argmax(np.abs(second_diffs)) + 2 + 1\n",
    "\n",
    "# Threshold method\n",
    "def find_n_components_variance(explained_variance_ratio, threshold=0.9):\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    return n_components\n",
    "\n",
    "# Apply both methods\n",
    "n_components_80 = find_n_components_variance(explained_variance, 0.8)\n",
    "n_components_90 = find_n_components_variance(explained_variance, 0.9)\n",
    "n_components_95 = find_n_components_variance(explained_variance, 0.95)\n",
    "n_components_elbow = find_n_components_elbow(explained_variance)\n",
    "\n",
    "print(f\"Number of components for 80% variance: {n_components_80}\")\n",
    "print(f\"Number of components for 90% variance: {n_components_90}\")\n",
    "print(f\"Number of components for 95% variance: {n_components_95}\")\n",
    "print(f\"Number of components using elbow method: {n_components_elbow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the \"elbow\" in the scree plot to better understand this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', linewidth=2, color='blue')\n",
    "plt.axvline(x=n_components_elbow, color='r', linestyle='--', \n",
    "           label=f'Elbow point: {n_components_elbow} components')\n",
    "plt.axvline(x=n_components_80, color='g', linestyle='--', \n",
    "           label=f'80% variance: {n_components_90} components')\n",
    "plt.axvline(x=n_components_90, color='b', linestyle='--', \n",
    "           label=f'90% variance: {n_components_90} components')\n",
    "plt.axvline(x=n_components_95, color='purple', linestyle='--', \n",
    "           label=f'95% variance: {n_components_95} components')\n",
    "plt.title('Scree Plot with Elbow Point')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing in 3D\n",
    "\n",
    "In papers, you'll typically see people only use two principal components to show clustering (often the first and second principal axes), but sometimes, authors will choose to use a **3D plot** instead. Below, we will demonstrate how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "df_pca = pd.DataFrame({\n",
    "    'PC1': X_pca_3d[:, 0],\n",
    "    'PC2': X_pca_3d[:, 1],\n",
    "    'PC3': X_pca_3d[:, 2],\n",
    "    'Class': y\n",
    "})\n",
    "\n",
    "# Create interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_pca,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    z='PC3',\n",
    "    color='Class',\n",
    "    title='3D PCA Projection',\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# Print explained variance for PCA\n",
    "print(f\"PCA explained variance ratio: {pca_3d.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained by 3D PCA: {sum(pca_3d.explained_variance_ratio_):.3f} or {sum(pca_3d.explained_variance_ratio_)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post-module notebook, we've explored methods for determining the optimal number of PCA components, specifically focusing on the elbow method and variance thresholds. We've also examined how the number of components affects the quality of data reconstruction.\n",
    "\n",
    "Key takeaways:\n",
    "- The elbow method offers a mathematically driven approach to finding the optimal number of components\n",
    "  - The elbow method can also be extended to other clustering-based algorithms as a heuristic for determining the number of clusters or components\n",
    "- Variance thresholds (e.g., 80%, 90%, 95%) provide practical cutoffs for dimensionality reduction\n",
    "- The optimal number of components depends on your specific application and requirements\n",
    "- Depending on your use case, you can  generate 3D plots for your PCA analysis\n",
    "\n",
    "These insights will help you apply PCA more effectively in your data analysis and machine learning projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
